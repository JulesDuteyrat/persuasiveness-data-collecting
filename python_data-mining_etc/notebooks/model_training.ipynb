{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "#pip install numpy\n",
    "import numpy as np\n",
    "#pip install pandas\n",
    "import pandas as pd\n",
    "#pip install --upgrade tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "#standard library\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function allows to read the csv files from the logs of the attentive cursor dataset\n",
    "#path_logs : path to the logs of the dataset with the files containing the list of inputs for each user\n",
    "#path_csv : path to the groundtruth.csv containing the attention score of the each user\n",
    "#dfs : array containing, for each user, its attention score and the count of its inputs\n",
    "def get_csv_data(path_logs, path_csv):\n",
    "    data = pd.read_csv(path_csv, sep='\\t')\n",
    "    dfs = []\n",
    "    for dirname, _, filenames in os.walk(path_logs):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.csv'):\n",
    "                file_path = os.path.join(dirname, filename).replace('\\\\', '/')\n",
    "                df = pd.read_csv(file_path,sep=' ')\n",
    "                \n",
    "                row = data.loc[data['log_id'] == int(filename[:-4])] # https://stackoverflow.com/questions/17071871/how-do-i-select-rows-from-a-dataframe-based-on-column-values\n",
    "                # print(row)\n",
    "                arr = row.values.tolist()\n",
    "                arr.append(df)\n",
    "                dfs.append(arr)\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the data\n",
    "dfs = get_csv_data('../the-attentive-cursor-dataset/logs', '../the-attentive-cursor-dataset/groundtruth.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeping the attention score and the amount of each input for each user\n",
    "cleandf = []\n",
    "for row in range(len(dfs)):\n",
    "    cleandf.append([dfs[row][0][2],dfs[row][1]['event'].value_counts()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['blur', 'click', 'contextmenu', 'copy', 'keydown', 'keyup', 'mousedown', 'mousemove', 'mouseover', 'resize', 'scroll', 'select', 'touchmove', 'touchstart']\n"
     ]
    }
   ],
   "source": [
    "#removing 'useless input types from the data'\n",
    "tab = np.array(cleandf, dtype='object')\n",
    "removed = ['mouseup','focus','load','beforeunload','unload','touchend'] #arbitrary list of input types to remove\n",
    "for row in tab :\n",
    "    for input in removed : \n",
    "        if input in row[1].keys() :\n",
    "            row[1].pop(input)\n",
    "\n",
    "#searching the data for every other input types\n",
    "all_inputs = []\n",
    "for row in tab :\n",
    "    for input in row[1].keys() :\n",
    "        if input not in all_inputs :\n",
    "            all_inputs.append(input)\n",
    "all_inputs.sort()\n",
    "print(all_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor input in all_inputs :\\n    max = 0\\n    for row in tab :\\n        if input in row[1].keys() :\\n            if row[1].get(input) > max :\\n                max = row[1].get(input)\\n    for row in tab :\\n        if input in row[1].keys() :\\n            row[1][input] = row[1][input]/max\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normaliations and the removal of extremes data are kept commented because they did not affect the accuracy\n",
    "#of the model in any significant way but are kept in the code for future devs to know what has been tried on\n",
    "#this model\n",
    "\n",
    "#normalisation v1\n",
    "\"\"\"\n",
    "for input in all_inputs :\n",
    "    tmp = []\n",
    "    for row in tab :\n",
    "        if input in row[1].keys() :\n",
    "            tmp.append(row[1].get(input))\n",
    "    mean = np.mean(tmp)\n",
    "    for row in tab :\n",
    "        if input in row[1].keys() :\n",
    "            row[1][input] = row[1][input]/mean\n",
    "\n",
    "print(tab)\n",
    "\"\"\"\n",
    "#normalisation v3\n",
    "\"\"\"\n",
    "for input in all_inputs :\n",
    "    max = 0\n",
    "    for row in tab :\n",
    "        if input in row[1].keys() :\n",
    "            if row[1].get(input) > max :\n",
    "                max = row[1].get(input)\n",
    "    for row in tab :\n",
    "        if input in row[1].keys() :\n",
    "            row[1][input] = row[1][input]/max\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 2 1 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [2 0 0 ... 0 0 0]\n",
      " ...\n",
      " [2 1 0 ... 0 0 0]\n",
      " [3 1 0 ... 0 0 0]\n",
      " [1 1 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "tmp_all_x = [] #stock a python array version of the data shaped as the entry of the model\n",
    "tmp_all_y = [] #stock a python array version of the data shaped as the results of the model (for training purpose)\n",
    "\n",
    "for row in tab :\n",
    "    tmp_all_x.append([])\n",
    "    for input in all_inputs :\n",
    "        if input in row[1].keys() :\n",
    "            tmp_all_x[len(tmp_all_x)-1].append(row[1].get(input))\n",
    "        else :\n",
    "            tmp_all_x[len(tmp_all_x)-1].append(0)\n",
    "    tmp_all_y.append(row[0])\n",
    "\n",
    "all_x = np.array(tmp_all_x) #numpy array version of the shaped data\n",
    "all_y = np.array(tmp_all_y)\n",
    "print(all_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ni = 0\\nbad_rows = []\\nfor row in range(len(all_x)) :\\n    for element in all_x[row] :\\n        if element > 5 :\\n            i +=1\\n            bad_rows.append(row)\\n            continue\\n\\nall_x = np.delete(all_x,bad_rows, axis=0)\\nprint(all_x)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalisation v2\n",
    "\"\"\"\n",
    "for i in range(len(all_inputs)) :\n",
    "    print(i)\n",
    "    tmp = []\n",
    "    for row in all_x :\n",
    "        tmp.append(row[i])\n",
    "    mean = np.mean(tmp)\n",
    "    for row in all_x :\n",
    "        row[i] = row[i]/mean\n",
    "    \n",
    "print(all_x)\n",
    "\"\"\"\n",
    "\n",
    "#enlevage des données extrèmes (faire une normalisation avant)\n",
    "\"\"\"\n",
    "i = 0\n",
    "bad_rows = []\n",
    "for row in range(len(all_x)) :\n",
    "    for element in all_x[row] :\n",
    "        if element > 5 :\n",
    "            i +=1\n",
    "            bad_rows.append(row)\n",
    "            continue\n",
    "\n",
    "all_x = np.delete(all_x,bad_rows, axis=0)\n",
    "print(all_x)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Keras symbolic inputs/outputs do not implement `__len__`. You may be trying to pass Keras symbolic inputs/outputs to a TF API that does not register dispatching, preventing Keras from automatically converting the API call to a lambda layer in the Functional Model. This error will also get raised if you try asserting a symbolic input/output directly.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jules\\OneDrive\\Documents\\measuring persuasiveness\\persuasiveness-data-collecting\\python_data-mining_etc\\notebooks\\model_training.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jules/OneDrive/Documents/measuring%20persuasiveness/persuasiveness-data-collecting/python_data-mining_etc/notebooks/model_training.ipynb#ch0000008?line=2'>3</a>\u001b[0m outputs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDense(\u001b[39m6\u001b[39m, activation\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39msoftmax)(x)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jules/OneDrive/Documents/measuring%20persuasiveness/persuasiveness-data-collecting/python_data-mining_etc/notebooks/model_training.ipynb#ch0000008?line=4'>5</a>\u001b[0m \u001b[39m#model = keras.Model(inputs=inputs, outputs = outputs)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jules/OneDrive/Documents/measuring%20persuasiveness/persuasiveness-data-collecting/python_data-mining_etc/notebooks/model_training.ipynb#ch0000008?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39;49mSequential(x)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jules/OneDrive/Documents/measuring%20persuasiveness/persuasiveness-data-collecting/python_data-mining_etc/notebooks/model_training.ipynb#ch0000008?line=7'>8</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jules/OneDrive/Documents/measuring%20persuasiveness/persuasiveness-data-collecting/python_data-mining_etc/notebooks/model_training.ipynb#ch0000008?line=8'>9</a>\u001b[0m     loss\u001b[39m=\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jules/OneDrive/Documents/measuring%20persuasiveness/persuasiveness-data-collecting/python_data-mining_etc/notebooks/model_training.ipynb#ch0000008?line=9'>10</a>\u001b[0m     optimizer\u001b[39m=\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mRMSprop(),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jules/OneDrive/Documents/measuring%20persuasiveness/persuasiveness-data-collecting/python_data-mining_etc/notebooks/model_training.ipynb#ch0000008?line=10'>11</a>\u001b[0m     metrics\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jules/OneDrive/Documents/measuring%20persuasiveness/persuasiveness-data-collecting/python_data-mining_etc/notebooks/model_training.ipynb#ch0000008?line=11'>12</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jules/OneDrive/Documents/measuring%20persuasiveness/persuasiveness-data-collecting/python_data-mining_etc/notebooks/model_training.ipynb#ch0000008?line=13'>14</a>\u001b[0m \u001b[39m#list of inputs (uncoment to see)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\training\\tracking\\base.py:587\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    586\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 587\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    588\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    589\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\keras_tensor.py:221\u001b[0m, in \u001b[0;36mKerasTensor.__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__len__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 221\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mKeras symbolic inputs/outputs do not \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    222\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mimplement `__len__`. You may be \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    223\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mtrying to pass Keras symbolic inputs/outputs \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    224\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mto a TF API that does not register dispatching, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    225\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mpreventing Keras from automatically \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    226\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mconverting the API call to a lambda layer \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    227\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39min the Functional Model. This error will also get raised \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    228\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mif you try asserting a symbolic input/output directly.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Keras symbolic inputs/outputs do not implement `__len__`. You may be trying to pass Keras symbolic inputs/outputs to a TF API that does not register dispatching, preventing Keras from automatically converting the API call to a lambda layer in the Functional Model. This error will also get raised if you try asserting a symbolic input/output directly."
     ]
    }
   ],
   "source": [
    "\n",
    "inputs = keras.Input(shape=(len(all_inputs),))\n",
    "x = tf.keras.layers.Dense(1, activation=tf.nn.relu)(inputs)\n",
    "outputs = tf.keras.layers.Dense(6, activation=tf.nn.softmax)(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs = outputs)\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.RMSprop(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "#list of inputs (uncoment to see)\n",
    "print('all used input types : ')\n",
    "print(all_inputs)\n",
    "#all_inputs correspond to all the inputs type minus\n",
    "print('all removed inputs : ')\n",
    "print(removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 0 1 9 3 0 7 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(all_x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "82/82 [==============================] - 1s 3ms/step - loss: 1.7867 - accuracy: 0.3587 - val_loss: 1.7825 - val_accuracy: 0.4261\n",
      "Epoch 2/20\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 1.7783 - accuracy: 0.3655 - val_loss: 1.7734 - val_accuracy: 0.4192\n",
      "Epoch 3/20\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 1.7698 - accuracy: 0.3652 - val_loss: 1.7642 - val_accuracy: 0.4192\n",
      "Epoch 4/20\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 1.7615 - accuracy: 0.3667 - val_loss: 1.7553 - val_accuracy: 0.4227\n",
      "Epoch 5/20\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 1.7533 - accuracy: 0.3648 - val_loss: 1.7465 - val_accuracy: 0.4227\n",
      "Epoch 6/20\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 1.7449 - accuracy: 0.3652 - val_loss: 1.7378 - val_accuracy: 0.4227\n",
      "Epoch 7/20\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 1.7365 - accuracy: 0.3652 - val_loss: 1.7283 - val_accuracy: 0.4227\n",
      "Epoch 8/20\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 1.7286 - accuracy: 0.3678 - val_loss: 1.7181 - val_accuracy: 0.4227\n",
      "Epoch 9/20\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 1.7207 - accuracy: 0.3671 - val_loss: 1.7084 - val_accuracy: 0.4227\n",
      "Epoch 10/20\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 1.7128 - accuracy: 0.3694 - val_loss: 1.6976 - val_accuracy: 0.4261\n",
      "Epoch 11/20\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 1.7032 - accuracy: 0.3697 - val_loss: 1.6866 - val_accuracy: 0.4261\n",
      "Epoch 12/20\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 1.6912 - accuracy: 0.3697 - val_loss: 1.6719 - val_accuracy: 0.4261\n",
      "Epoch 13/20\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 1.6767 - accuracy: 0.3697 - val_loss: 1.6503 - val_accuracy: 0.4261\n",
      "Epoch 14/20\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 1.6626 - accuracy: 0.3697 - val_loss: 1.6340 - val_accuracy: 0.4261\n",
      "Epoch 15/20\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 1.6539 - accuracy: 0.3697 - val_loss: 1.6257 - val_accuracy: 0.4261\n",
      "Epoch 16/20\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 1.6512 - accuracy: 0.3697 - val_loss: 1.6239 - val_accuracy: 0.4261\n",
      "Epoch 17/20\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 1.6501 - accuracy: 0.3697 - val_loss: 1.6225 - val_accuracy: 0.4261\n",
      "Epoch 18/20\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 1.6495 - accuracy: 0.3697 - val_loss: 1.6224 - val_accuracy: 0.4261\n",
      "Epoch 19/20\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 1.6492 - accuracy: 0.3697 - val_loss: 1.6226 - val_accuracy: 0.4261\n",
      "Epoch 20/20\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 1.6489 - accuracy: 0.3697 - val_loss: 1.6228 - val_accuracy: 0.4261\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(all_x, all_y, epochs=20, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflowjs as tfjs\n",
    "tfjs.converters.save_keras_model(model, './model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 66ms/step\n",
      "[[0.0000000e+00 3.4934153e-33 6.9748394e-25 0.0000000e+00 9.7245544e-01\n",
      "  2.7544534e-02]]\n"
     ]
    }
   ],
   "source": [
    "num_array = np.array([ 0, 17, 0, 0, 0, 0, 21, 324, 28, 0, 0, 0, 0, 0 ])\n",
    "prediction = model.predict(num_array.reshape(1, -1))\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization_8 (Normalizat  (None, 14)               3         \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1)                 15        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18\n",
      "Trainable params: 15\n",
      "Non-trainable params: 3\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#attempt at a linear reggression model\n",
    "data = all_x\n",
    "\n",
    "data_normalizer = keras.layers.Normalization(input_shape=[14,], axis=None)\n",
    "data_normalizer.adapt(data)\n",
    "\n",
    "persuasiveness_model = tf.keras.Sequential([\n",
    "    data_normalizer,\n",
    "    keras.layers.Dense(units=1)\n",
    "])\n",
    "\n",
    "persuasiveness_model.summary()\n",
    "\n",
    "persuasiveness_model.compile(\n",
    "    optimizer=tf.optimizers.Adam(learning_rate=0.1),\n",
    "    loss='mean_absolute_error',\n",
    "    metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 1.0772 - mae: 1.0772 - val_loss: 1.0225 - val_mae: 1.0225\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 1.0645 - mae: 1.0645 - val_loss: 0.9613 - val_mae: 0.9613\n",
      "Epoch 3/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0753 - mae: 1.0753 - val_loss: 0.9420 - val_mae: 0.9420\n",
      "Epoch 4/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0732 - mae: 1.0732 - val_loss: 0.9922 - val_mae: 0.9922\n",
      "Epoch 5/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0764 - mae: 1.0764 - val_loss: 0.9594 - val_mae: 0.9594\n",
      "Epoch 6/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0794 - mae: 1.0794 - val_loss: 0.9835 - val_mae: 0.9835\n",
      "Epoch 7/100\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 1.0813 - mae: 1.0813 - val_loss: 0.9630 - val_mae: 0.9630\n",
      "Epoch 8/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0623 - mae: 1.0623 - val_loss: 0.9343 - val_mae: 0.9343\n",
      "Epoch 9/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0538 - mae: 1.0538 - val_loss: 1.0311 - val_mae: 1.0311\n",
      "Epoch 10/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0741 - mae: 1.0741 - val_loss: 0.9488 - val_mae: 0.9488\n",
      "Epoch 11/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0590 - mae: 1.0590 - val_loss: 0.9551 - val_mae: 0.9551\n",
      "Epoch 12/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0780 - mae: 1.0780 - val_loss: 1.0141 - val_mae: 1.0141\n",
      "Epoch 13/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.1069 - mae: 1.1069 - val_loss: 0.9964 - val_mae: 0.9964\n",
      "Epoch 14/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0563 - mae: 1.0563 - val_loss: 0.9988 - val_mae: 0.9988\n",
      "Epoch 15/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0477 - mae: 1.0477 - val_loss: 0.9338 - val_mae: 0.9338\n",
      "Epoch 16/100\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 1.0682 - mae: 1.0682 - val_loss: 0.9652 - val_mae: 0.9652\n",
      "Epoch 17/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0709 - mae: 1.0709 - val_loss: 0.9438 - val_mae: 0.9438\n",
      "Epoch 18/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0469 - mae: 1.0469 - val_loss: 0.9648 - val_mae: 0.9648\n",
      "Epoch 19/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0705 - mae: 1.0705 - val_loss: 0.9551 - val_mae: 0.9551\n",
      "Epoch 20/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0735 - mae: 1.0735 - val_loss: 0.9926 - val_mae: 0.9926\n",
      "Epoch 21/100\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 1.0904 - mae: 1.0904 - val_loss: 1.0523 - val_mae: 1.0523\n",
      "Epoch 22/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0791 - mae: 1.0791 - val_loss: 0.9522 - val_mae: 0.9522\n",
      "Epoch 23/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0495 - mae: 1.0495 - val_loss: 0.9258 - val_mae: 0.9258\n",
      "Epoch 24/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0686 - mae: 1.0686 - val_loss: 1.0018 - val_mae: 1.0018\n",
      "Epoch 25/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.1083 - mae: 1.1083 - val_loss: 1.0387 - val_mae: 1.0387\n",
      "Epoch 26/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0536 - mae: 1.0536 - val_loss: 0.9362 - val_mae: 0.9362\n",
      "Epoch 27/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0778 - mae: 1.0778 - val_loss: 0.9904 - val_mae: 0.9904\n",
      "Epoch 28/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0840 - mae: 1.0840 - val_loss: 1.0383 - val_mae: 1.0383\n",
      "Epoch 29/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.1095 - mae: 1.1095 - val_loss: 0.9690 - val_mae: 0.9690\n",
      "Epoch 30/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0662 - mae: 1.0662 - val_loss: 0.9295 - val_mae: 0.9295\n",
      "Epoch 31/100\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 1.0772 - mae: 1.0772 - val_loss: 0.9775 - val_mae: 0.9775\n",
      "Epoch 32/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0468 - mae: 1.0468 - val_loss: 1.0035 - val_mae: 1.0035\n",
      "Epoch 33/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0641 - mae: 1.0641 - val_loss: 0.9461 - val_mae: 0.9461\n",
      "Epoch 34/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0805 - mae: 1.0805 - val_loss: 1.0386 - val_mae: 1.0386\n",
      "Epoch 35/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0778 - mae: 1.0778 - val_loss: 0.9850 - val_mae: 0.9850\n",
      "Epoch 36/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0695 - mae: 1.0695 - val_loss: 0.9503 - val_mae: 0.9503\n",
      "Epoch 37/100\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 1.0460 - mae: 1.0460 - val_loss: 1.0448 - val_mae: 1.0448\n",
      "Epoch 38/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0638 - mae: 1.0638 - val_loss: 0.9550 - val_mae: 0.9550\n",
      "Epoch 39/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0488 - mae: 1.0488 - val_loss: 1.0127 - val_mae: 1.0127\n",
      "Epoch 40/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0821 - mae: 1.0821 - val_loss: 0.9362 - val_mae: 0.9362\n",
      "Epoch 41/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0820 - mae: 1.0820 - val_loss: 1.0551 - val_mae: 1.0551\n",
      "Epoch 42/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0804 - mae: 1.0804 - val_loss: 0.9973 - val_mae: 0.9973\n",
      "Epoch 43/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0598 - mae: 1.0598 - val_loss: 0.9435 - val_mae: 0.9435\n",
      "Epoch 44/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0730 - mae: 1.0730 - val_loss: 0.9698 - val_mae: 0.9698\n",
      "Epoch 45/100\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 1.0768 - mae: 1.0768 - val_loss: 0.9807 - val_mae: 0.9807\n",
      "Epoch 46/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0435 - mae: 1.0435 - val_loss: 0.9437 - val_mae: 0.9437\n",
      "Epoch 47/100\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 1.0653 - mae: 1.0653 - val_loss: 0.9444 - val_mae: 0.9444\n",
      "Epoch 48/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0731 - mae: 1.0731 - val_loss: 0.9726 - val_mae: 0.9726\n",
      "Epoch 49/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0654 - mae: 1.0654 - val_loss: 0.9601 - val_mae: 0.9601\n",
      "Epoch 50/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0561 - mae: 1.0561 - val_loss: 0.9484 - val_mae: 0.9484\n",
      "Epoch 51/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0524 - mae: 1.0524 - val_loss: 0.9588 - val_mae: 0.9588\n",
      "Epoch 52/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0639 - mae: 1.0639 - val_loss: 1.1237 - val_mae: 1.1237\n",
      "Epoch 53/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0767 - mae: 1.0767 - val_loss: 1.1028 - val_mae: 1.1028\n",
      "Epoch 54/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0817 - mae: 1.0817 - val_loss: 0.9407 - val_mae: 0.9407\n",
      "Epoch 55/100\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 1.0735 - mae: 1.0735 - val_loss: 0.9850 - val_mae: 0.9850\n",
      "Epoch 56/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0603 - mae: 1.0603 - val_loss: 0.9568 - val_mae: 0.9568\n",
      "Epoch 57/100\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 1.0666 - mae: 1.0666 - val_loss: 0.9665 - val_mae: 0.9665\n",
      "Epoch 58/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0560 - mae: 1.0560 - val_loss: 0.9333 - val_mae: 0.9333\n",
      "Epoch 59/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0852 - mae: 1.0852 - val_loss: 1.0219 - val_mae: 1.0219\n",
      "Epoch 60/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0558 - mae: 1.0558 - val_loss: 1.0260 - val_mae: 1.0260\n",
      "Epoch 61/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0678 - mae: 1.0678 - val_loss: 0.9868 - val_mae: 0.9868\n",
      "Epoch 62/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0638 - mae: 1.0638 - val_loss: 0.9215 - val_mae: 0.9215\n",
      "Epoch 63/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0685 - mae: 1.0685 - val_loss: 0.9602 - val_mae: 0.9602\n",
      "Epoch 64/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0740 - mae: 1.0740 - val_loss: 1.0496 - val_mae: 1.0496\n",
      "Epoch 65/100\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 1.0583 - mae: 1.0583 - val_loss: 0.9930 - val_mae: 0.9930\n",
      "Epoch 66/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0452 - mae: 1.0452 - val_loss: 0.9332 - val_mae: 0.9332\n",
      "Epoch 67/100\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 1.1170 - mae: 1.1170 - val_loss: 1.0064 - val_mae: 1.0064\n",
      "Epoch 68/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0641 - mae: 1.0641 - val_loss: 0.9310 - val_mae: 0.9310\n",
      "Epoch 69/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0568 - mae: 1.0568 - val_loss: 0.9463 - val_mae: 0.9463\n",
      "Epoch 70/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0841 - mae: 1.0841 - val_loss: 0.9810 - val_mae: 0.9810\n",
      "Epoch 71/100\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 1.0689 - mae: 1.0689 - val_loss: 0.9462 - val_mae: 0.9462\n",
      "Epoch 72/100\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 1.0658 - mae: 1.0658 - val_loss: 1.0246 - val_mae: 1.0246\n",
      "Epoch 73/100\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 1.0620 - mae: 1.0620 - val_loss: 0.9281 - val_mae: 0.9281\n",
      "Epoch 74/100\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 1.0740 - mae: 1.0740 - val_loss: 1.0677 - val_mae: 1.0677\n",
      "Epoch 75/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0824 - mae: 1.0824 - val_loss: 0.9750 - val_mae: 0.9750\n",
      "Epoch 76/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0713 - mae: 1.0713 - val_loss: 0.9755 - val_mae: 0.9755\n",
      "Epoch 77/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0516 - mae: 1.0516 - val_loss: 0.9349 - val_mae: 0.9349\n",
      "Epoch 78/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0509 - mae: 1.0509 - val_loss: 0.9942 - val_mae: 0.9942\n",
      "Epoch 79/100\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 1.0837 - mae: 1.0837 - val_loss: 0.9786 - val_mae: 0.9786\n",
      "Epoch 80/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0889 - mae: 1.0889 - val_loss: 0.9527 - val_mae: 0.9527\n",
      "Epoch 81/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0813 - mae: 1.0813 - val_loss: 0.9547 - val_mae: 0.9547\n",
      "Epoch 82/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0464 - mae: 1.0464 - val_loss: 0.9627 - val_mae: 0.9627\n",
      "Epoch 83/100\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 1.0643 - mae: 1.0643 - val_loss: 0.9522 - val_mae: 0.9522\n",
      "Epoch 84/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0748 - mae: 1.0748 - val_loss: 1.0903 - val_mae: 1.0903\n",
      "Epoch 85/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0603 - mae: 1.0603 - val_loss: 0.9330 - val_mae: 0.9330\n",
      "Epoch 86/100\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 1.0491 - mae: 1.0491 - val_loss: 1.0444 - val_mae: 1.0444\n",
      "Epoch 87/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0727 - mae: 1.0727 - val_loss: 0.9470 - val_mae: 0.9470\n",
      "Epoch 88/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0653 - mae: 1.0653 - val_loss: 0.9821 - val_mae: 0.9821\n",
      "Epoch 89/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0537 - mae: 1.0537 - val_loss: 0.9993 - val_mae: 0.9993\n",
      "Epoch 90/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0657 - mae: 1.0657 - val_loss: 1.1822 - val_mae: 1.1822\n",
      "Epoch 91/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0859 - mae: 1.0859 - val_loss: 0.9594 - val_mae: 0.9594\n",
      "Epoch 92/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0637 - mae: 1.0637 - val_loss: 0.9888 - val_mae: 0.9888\n",
      "Epoch 93/100\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 1.0557 - mae: 1.0557 - val_loss: 0.9501 - val_mae: 0.9501\n",
      "Epoch 94/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0406 - mae: 1.0406 - val_loss: 0.9467 - val_mae: 0.9467\n",
      "Epoch 95/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0555 - mae: 1.0555 - val_loss: 1.0207 - val_mae: 1.0207\n",
      "Epoch 96/100\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 1.0646 - mae: 1.0646 - val_loss: 0.9627 - val_mae: 0.9627\n",
      "Epoch 97/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0453 - mae: 1.0453 - val_loss: 0.9756 - val_mae: 0.9756\n",
      "Epoch 98/100\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 1.0550 - mae: 1.0550 - val_loss: 0.9389 - val_mae: 0.9389\n",
      "Epoch 99/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0487 - mae: 1.0487 - val_loss: 1.0304 - val_mae: 1.0304\n",
      "Epoch 100/100\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0894 - mae: 1.0894 - val_loss: 0.9710 - val_mae: 0.9710\n"
     ]
    }
   ],
   "source": [
    "history = persuasiveness_model.fit(all_x, all_y, epochs=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.7076702],\n",
       "       [3.963278 ],\n",
       "       [4.0995793],\n",
       "       [4.1615567],\n",
       "       [4.064081 ],\n",
       "       [4.1151037],\n",
       "       [4.0078044],\n",
       "       [4.062291 ],\n",
       "       [3.99364  ],\n",
       "       [3.944041 ]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persuasiveness_model.predict(data[:10])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
